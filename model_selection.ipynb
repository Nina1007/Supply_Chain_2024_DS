{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.metrics import classification_report_imbalanced, geometric_mean_score\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler,  ClusterCentroids\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn import ensemble, linear_model, preprocessing, neighbors, datasets\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, VotingClassifier, StackingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report, roc_curve, auc, balanced_accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_validate, KFold, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (80209, 19)\n",
      "X_test shape: (20053, 19)\n",
      "y_train shape: (80209,)\n",
      "y_test shape: (20053,)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 80209 entries, 0 to 80208\n",
      "Data columns (total 19 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Sentiment_Blob       80209 non-null  float64\n",
      " 1   Sentiment_VADER      80209 non-null  float64\n",
      " 2   Sentiment_VADER_cat  80209 non-null  int64  \n",
      " 3   Sentiment_Blob_cat   80209 non-null  float64\n",
      " 4   text_word_length     80209 non-null  int64  \n",
      " 5   text_length          80209 non-null  int64  \n",
      " 6   tfidf_331            80209 non-null  float64\n",
      " 7   tfidf_1867           80209 non-null  float64\n",
      " 8   tfidf_3522           80209 non-null  float64\n",
      " 9   tfidf_4431           80209 non-null  float64\n",
      " 10  bow_331              80209 non-null  int64  \n",
      " 11  bow_1739             80209 non-null  int64  \n",
      " 12  bow_2738             80209 non-null  int64  \n",
      " 13  bow_4431             80209 non-null  int64  \n",
      " 14  bow_1867             80209 non-null  int64  \n",
      " 15  bow_3522             80209 non-null  int64  \n",
      " 16  bow_1233             80209 non-null  int64  \n",
      " 17  bow_402              80209 non-null  int64  \n",
      " 18  bow_4134             80209 non-null  int64  \n",
      "dtypes: float64(7), int64(12)\n",
      "memory usage: 11.6 MB\n",
      "X_train info: None\n",
      "X_train dtypes: Sentiment_Blob         float64\n",
      "Sentiment_VADER        float64\n",
      "Sentiment_VADER_cat      int64\n",
      "Sentiment_Blob_cat     float64\n",
      "text_word_length         int64\n",
      "text_length              int64\n",
      "tfidf_331              float64\n",
      "tfidf_1867             float64\n",
      "tfidf_3522             float64\n",
      "tfidf_4431             float64\n",
      "bow_331                  int64\n",
      "bow_1739                 int64\n",
      "bow_2738                 int64\n",
      "bow_4431                 int64\n",
      "bow_1867                 int64\n",
      "bow_3522                 int64\n",
      "bow_1233                 int64\n",
      "bow_402                  int64\n",
      "bow_4134                 int64\n",
      "dtype: object\n",
      "X_train shape: (80209, 6)\n",
      "X_test shape: (20053, 6)\n",
      "y_train shape: (80209,)\n",
      "y_test shape: (20053,)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 80209 entries, 0 to 80208\n",
      "Data columns (total 6 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   text_word_length_scaled          80209 non-null  float64\n",
      " 1   Sentiment_Blob                   80209 non-null  float64\n",
      " 2   verification_encoded_normalized  80209 non-null  float64\n",
      " 3   number_reviews_scaled            80209 non-null  float64\n",
      " 4   Sentiment_VADER                  80209 non-null  float64\n",
      " 5   location_encoded_normalized      80209 non-null  float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 3.7 MB\n",
      "X_train info: None\n",
      "X_train dtypes: text_word_length_scaled            float64\n",
      "Sentiment_Blob                     float64\n",
      "verification_encoded_normalized    float64\n",
      "number_reviews_scaled              float64\n",
      "Sentiment_VADER                    float64\n",
      "location_encoded_normalized        float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Loading the datasets - correlation-based feature selection\n",
    "X_train_correlation = pd.read_csv('data/X_train_correlation.csv', engine='python')\n",
    "X_test_correlation = pd.read_csv('data/X_test_correlation.csv', engine='python')\n",
    "y_train_correlation = pd.read_csv('data/y_train_correlation.csv', engine='python')['rating']\n",
    "y_test_correlation = pd.read_csv('data/y_test_correlation.csv', engine='python')['rating']\n",
    "\n",
    "print(f\"X_train shape: {X_train_correlation.shape}\")\n",
    "print(f\"X_test shape: {X_test_correlation.shape}\")\n",
    "print(f\"y_train shape: {y_train_correlation.shape}\")\n",
    "print(f\"y_test shape: {y_test_correlation.shape}\")\n",
    "\n",
    "print(\"X_train info:\", X_train_correlation.info())\n",
    "print(\"X_train dtypes:\", X_train_correlation.dtypes)   \n",
    "\n",
    "#Loading the datasets - regular feature selection\n",
    "X_train = pd.read_csv('data/X_train.csv', engine='python')\n",
    "X_test = pd.read_csv('data/X_test.csv', engine='python')\n",
    "y_train = pd.read_csv('data/y_train.csv', engine='python')['rating']\n",
    "y_test = pd.read_csv('data/y_test.csv', engine='python')['rating']\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "print(\"X_train info:\", X_train.info())\n",
    "print(\"X_train dtypes:\", X_train.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def apply_resampling(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    method='none',\n",
    "    random_state=25,\n",
    "    n_jobs=-1\n",
    "):\n",
    "    \"\"\"\n",
    "    Simplified resampling function including natural breaks\n",
    "    \"\"\"\n",
    "    if method == 'none':\n",
    "        return X_train, y_train, {\n",
    "            'method': None,\n",
    "            'original_shape': X_train.shape,\n",
    "            'original_distribution': pd.Series(y_train).value_counts().to_dict()\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        if method == 'natural_breaks':\n",
    "            # Implement natural breaks using class frequencies\n",
    "            class_counts = pd.Series(y_train).value_counts()\n",
    "            median_count = class_counts.median()\n",
    "            \n",
    "            # Sample or truncate each class to median size\n",
    "            resampled_data = []\n",
    "            resampled_labels = []\n",
    "            \n",
    "            for class_label in class_counts.index:\n",
    "                class_mask = y_train == class_label\n",
    "                class_data = X_train[class_mask]\n",
    "                \n",
    "                if len(class_data) > median_count:\n",
    "                    # Undersample to median\n",
    "                    indices = np.random.choice(len(class_data), size=int(median_count), replace=False)\n",
    "                    resampled_data.append(class_data.iloc[indices])\n",
    "                    resampled_labels.extend([class_label] * int(median_count))\n",
    "                else:\n",
    "                    # Keep all samples for smaller classes\n",
    "                    resampled_data.append(class_data)\n",
    "                    resampled_labels.extend([class_label] * len(class_data))\n",
    "            \n",
    "            X_resampled = pd.concat(resampled_data, axis=0)\n",
    "            y_resampled = pd.Series(resampled_labels)\n",
    "            \n",
    "        elif method == 'smote':\n",
    "            resampler = SMOTE(random_state=random_state, n_jobs=n_jobs)\n",
    "            X_resampled, y_resampled = resampler.fit_resample(X_train, y_train)\n",
    "        elif method == 'random_over':\n",
    "            resampler = RandomOverSampler(random_state=random_state)\n",
    "            X_resampled, y_resampled = resampler.fit_resample(X_train, y_train)\n",
    "        elif method == 'random_under':\n",
    "            resampler = RandomUnderSampler(random_state=random_state)\n",
    "            X_resampled, y_resampled = resampler.fit_resample(X_train, y_train)\n",
    "        elif method == 'cluster_centroids':\n",
    "            resampler = ClusterCentroids(random_state=random_state, n_jobs=n_jobs)\n",
    "            X_resampled, y_resampled = resampler.fit_resample(X_train, y_train)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown resampling method: {method}\")\n",
    "\n",
    "        return X_resampled, y_resampled, {\n",
    "            'method': method,\n",
    "            'original_shape': X_train.shape,\n",
    "            'resampled_shape': X_resampled.shape,\n",
    "            'original_distribution': pd.Series(y_train).value_counts().to_dict(),\n",
    "            'resampled_distribution': pd.Series(y_resampled).value_counts().to_dict()\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in {method} resampling: {str(e)}\")\n",
    "        return X_train, y_train, {'error': str(e)}\n",
    "\n",
    "def process_data_combination(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    use_anomaly_detection=False,\n",
    "    propagate_anomaly_data=False,\n",
    "    random_state=25\n",
    "):\n",
    "    \"\"\"\n",
    "    Process a single data combination with options for anomaly detection\n",
    "    and data propagation.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    if use_anomaly_detection:\n",
    "        # Simple IQR-based anomaly detection\n",
    "        Q1 = X_train.quantile(0.25)\n",
    "        Q3 = X_train.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Create mask for non-anomalous data\n",
    "        mask = ~((X_train < (Q1 - 1.5 * IQR)) | \n",
    "                (X_train > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "        \n",
    "        # Remove anomalies\n",
    "        X_train_clean = X_train[mask]\n",
    "        y_train_clean = y_train[mask]\n",
    "        \n",
    "        if propagate_anomaly_data:\n",
    "            # Add copies of minority class non-anomalous samples\n",
    "            for class_label in range(1, 6):  # 5 classes\n",
    "                class_mask = y_train_clean == class_label\n",
    "                if sum(class_mask) < len(y_train_clean) / 5:  # If minority class\n",
    "                    n_copies = int(len(y_train_clean) / 5 / sum(class_mask))\n",
    "                    for _ in range(n_copies - 1):\n",
    "                        X_train_clean = pd.concat([X_train_clean, X_train_clean[class_mask]])\n",
    "                        y_train_clean = pd.concat([y_train_clean, y_train_clean[class_mask]])\n",
    "        \n",
    "        return X_train_clean, X_test, y_train_clean, y_test\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_models(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    random_state=25,\n",
    "    n_jobs=-1\n",
    "):\n",
    "    \"\"\"\n",
    "    Train models with minimal hyperparameters.\n",
    "    Only essential parameters kept: random_state and n_jobs for parallelization\n",
    "    \"\"\"\n",
    "    # Convert classes to 0-based indexing for XGBoost\n",
    "    y_train_zero_based = y_train - 1\n",
    "    y_test_zero_based = y_test - 1\n",
    "    \n",
    "    # Create MinMaxScaler for NaiveBayes (ensures non-negative values)\n",
    "    nb_scaler = MinMaxScaler()\n",
    "    X_train_nb = nb_scaler.fit_transform(X_train)\n",
    "    X_test_nb = nb_scaler.transform(X_test)\n",
    "    \n",
    "    models = {\n",
    "        'logistic': (LogisticRegression(random_state=random_state), \n",
    "                    (X_train, X_test, y_train, y_test)),\n",
    "        'naive_bayes': (MultinomialNB(), \n",
    "                       (X_train_nb, X_test_nb, y_train, y_test)),  # Use scaled non-negative data\n",
    "        'decision_tree': (DecisionTreeClassifier(random_state=random_state),\n",
    "                         (X_train, X_test, y_train, y_test)),\n",
    "        'random_forest': (RandomForestClassifier(random_state=random_state, n_jobs=n_jobs),\n",
    "                         (X_train, X_test, y_train, y_test)),\n",
    "        'balanced_rf': (BalancedRandomForestClassifier(random_state=random_state, n_jobs=n_jobs),\n",
    "                       (X_train, X_test, y_train, y_test)),\n",
    "        'xgboost': (XGBClassifier(random_state=random_state, n_jobs=n_jobs),\n",
    "                    (X_train, X_test, y_train_zero_based, y_test_zero_based)),  # Use zero-based classes\n",
    "        'gradient_boosting': (GradientBoostingClassifier(random_state=random_state),\n",
    "                            (X_train, X_test, y_train, y_test)),\n",
    "        'adaboost': (AdaBoostClassifier(random_state=random_state),\n",
    "                    (X_train, X_test, y_train, y_test)),\n",
    "        'voting': (VotingClassifier(\n",
    "            estimators=[\n",
    "                ('lr', LogisticRegression(random_state=random_state)),\n",
    "                ('rf', RandomForestClassifier(random_state=random_state)),\n",
    "                ('gb', GradientBoostingClassifier(random_state=random_state))\n",
    "            ],\n",
    "            voting='soft',\n",
    "            n_jobs=n_jobs\n",
    "        ), (X_train, X_test, y_train, y_test)),\n",
    "        'bagging': (BaggingClassifier(\n",
    "            random_state=random_state,\n",
    "            n_jobs=n_jobs\n",
    "        ), (X_train, X_test, y_train, y_test))\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, (model, (X_tr, X_te, y_tr, y_te)) in models.items():\n",
    "        try:\n",
    "            # Train and predict\n",
    "            model.fit(X_tr, y_tr)\n",
    "            y_pred = model.predict(X_te)\n",
    "            \n",
    "            # Convert predictions back to 1-based if using XGBoost\n",
    "            if name == 'xgboost':\n",
    "                y_pred = y_pred + 1\n",
    "                y_te = y_test  # Use original labels for evaluation\n",
    "            \n",
    "            # Calculate metrics\n",
    "            f1_per_class = f1_score(y_te, y_pred, average=None)\n",
    "            results[name] = {\n",
    "                'f1_overall': f1_score(y_te, y_pred, average='weighted'),\n",
    "                'f1_per_class': f1_per_class.tolist(),\n",
    "                'f1_min': float(min(f1_per_class)),\n",
    "                'confusion_matrix': confusion_matrix(y_te, y_pred).tolist()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error training {name}: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_pipeline(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    X_train_corr,\n",
    "    X_test_corr,\n",
    "    y_train_corr,\n",
    "    y_test_corr,\n",
    "    random_state=25\n",
    "):\n",
    "    \"\"\"\n",
    "    Run complete pipeline testing all combinations.\n",
    "    Returns best model based on both overall and minimum F1 scores.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    all_results = {}\n",
    "    best_overall_f1 = 0\n",
    "    best_min_f1 = 0\n",
    "    best_combination = None\n",
    "    \n",
    "    # Dataset combinations\n",
    "    datasets = {\n",
    "        'regular': (X_train, X_test, y_train, y_test),\n",
    "        'correlation': (X_train_corr, X_test_corr, y_train_corr, y_test_corr)\n",
    "    }\n",
    "    \n",
    "    # Calculate total combinations for progress bar\n",
    "    n_datasets = len(datasets)\n",
    "    n_anomaly = 2  # [False, True]\n",
    "    n_propagate = 2  # [False, True] when anomaly=True\n",
    "    n_resampling = 6  # [none, smote, random_over, random_under, cluster_centroids, natural_breaks]\n",
    "    n_models = 10  # number of models in train_models\n",
    "    total_combinations = n_datasets * (1 + n_propagate) * n_resampling * n_models\n",
    "    \n",
    "    # Initialize progress bar\n",
    "    pbar = tqdm(total=total_combinations, desc=\"Training Models\")\n",
    "    \n",
    "    # Test all combinations\n",
    "    for dataset_name, (X_tr, X_te, y_tr, y_te) in datasets.items():\n",
    "        for anomaly in [False, True]:\n",
    "            for propagate in [False, True] if anomaly else [False]:\n",
    "                # Process data\n",
    "                X_train_processed, X_test_processed, y_train_processed, y_test_processed = process_data_combination(\n",
    "                    X_tr, X_te, y_tr, y_te,\n",
    "                    use_anomaly_detection=anomaly,\n",
    "                    propagate_anomaly_data=propagate,\n",
    "                    random_state=random_state\n",
    "                )\n",
    "                \n",
    "                # Try each resampling method\n",
    "                for resample_method in ['none', 'smote', 'random_over', 'random_under', 'cluster_centroids', 'natural_breaks']:\n",
    "                    X_train_resampled, y_train_resampled, resample_info = apply_resampling(\n",
    "                        X_train_processed,\n",
    "                        y_train_processed,\n",
    "                        method=resample_method,\n",
    "                        random_state=random_state\n",
    "                    )\n",
    "                    \n",
    "                    # Train all models\n",
    "                    model_results = train_models(\n",
    "                        X_train_resampled,\n",
    "                        X_test_processed,\n",
    "                        y_train_resampled,\n",
    "                        y_test_processed,\n",
    "                        random_state=random_state\n",
    "                    )\n",
    "                    \n",
    "                    # Store results\n",
    "                    combination_name = f\"{dataset_name}_anomaly{anomaly}_propagate{propagate}_{resample_method}\"\n",
    "                    all_results[combination_name] = model_results\n",
    "                    \n",
    "                    # Update best combination\n",
    "                    for model_name, metrics in model_results.items():\n",
    "                        if metrics['f1_min'] > best_min_f1 or (\n",
    "                            metrics['f1_min'] == best_min_f1 and \n",
    "                            metrics['f1_overall'] > best_overall_f1\n",
    "                        ):\n",
    "                            best_min_f1 = metrics['f1_min']\n",
    "                            best_overall_f1 = metrics['f1_overall']\n",
    "                            best_combination = {\n",
    "                                'dataset': dataset_name,\n",
    "                                'anomaly_detection': anomaly,\n",
    "                                'data_propagation': propagate,\n",
    "                                'resampling': resample_method,\n",
    "                                'model': model_name,\n",
    "                                'metrics': metrics\n",
    "                            }\n",
    "                        pbar.update(1)  # Update progress bar after each model\n",
    "    \n",
    "    pbar.close()  # Close progress bar\n",
    "    \n",
    "    return {\n",
    "        'all_results': all_results,\n",
    "        'best_combination': best_combination\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "results = run_pipeline(\n",
    "    X_train, X_test, y_train, y_test,\n",
    "    X_train_correlation, X_test_correlation,\n",
    "    y_train_correlation, y_test_correlation\n",
    ")\n",
    "\n",
    "# Print best combination\n",
    "best = results['best_combination']\n",
    "print(\"\\nBest Model Combination:\")\n",
    "print(f\"Dataset: {best['dataset']}\")\n",
    "print(f\"Anomaly Detection: {best['anomaly_detection']}\")\n",
    "print(f\"Data Propagation: {best['data_propagation']}\")\n",
    "print(f\"Resampling Method: {best['resampling']}\")\n",
    "print(f\"Model: {best['model']}\")\n",
    "print(f\"Overall F1: {best['metrics']['f1_overall']:.4f}\")\n",
    "print(f\"Minimum Class F1: {best['metrics']['f1_min']:.4f}\")\n",
    "print(\"Per-class F1 scores:\", best['metrics']['f1_per_class'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
