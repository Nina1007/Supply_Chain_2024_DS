{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this code ran on google colab. the execution time on a regular CPU will be very long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SimpleRNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "df = pd.read_csv(io.BytesIO(uploaded['data_trustpilot.csv']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df[\"rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the pre-trained sentence transformer model\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Extract the text data (preprocessed Text) and labels (ratings)\n",
    "X = df['text_processed'].tolist()\n",
    "y = df['rating'].tolist()\n",
    "\n",
    "# Encode the text data into sentence embeddings\n",
    "X_embeddings = model.encode(X)\n",
    "\n",
    "# Apply SMOTE to oversample minority classes\n",
    "smote = SMOTE()\n",
    "texts, ratings = smote.fit_resample(X_embeddings, y)\n",
    "\n",
    "# The result of resampling is now in `texts` and `ratings`\n",
    "\n",
    "# new class distribution after resampling\n",
    "print(pd.Series(ratings).value_counts())\n",
    "\n",
    "import pickle\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "# Save the SMOTE output\n",
    "with open('/content/drive/MyDrive/smote_output.pkl', 'wb') as f:\n",
    "    pickle.dump([texts, ratings], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Open the file in read binary ('rb') mode\n",
    "with open('/content/drive/MyDrive/smote_output.pkl', 'rb') as f:\n",
    "    # Load the data using pickle.load()\n",
    "    texts, ratings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SimpleRNN\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "texts = np.array(texts)  # Convert to NumPy array\n",
    "texts = texts.reshape(texts.shape[0], 1, texts.shape[1])  # Reshape to (samples, 1, features)\n",
    "\n",
    "# Explicitly convert the data type to float32\n",
    "texts = texts.astype(np.float32)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on ratings data\n",
    "encoder.fit(ratings)\n",
    "\n",
    "# Transform the training and testing labels\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "# Ensure data type consistency\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "\n",
    "# LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2, input_shape=(texts.shape[1], texts.shape[2])))  # Input shape matches embeddings\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=5, activation='softmax'))  # 5 classes for 5-star ratings\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n",
    "\n",
    "# Evaluate the Model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Validation Accuracy: \", accuracy * 100, \"%\")\n",
    "\n",
    "# Save the model\n",
    "model_save_path = '/content/drive/MyDrive/rnn_glove_rating_SMOTE_processed.h5'\n",
    "model_save_path_1 = '/content/drive/MyDrive/rnn_glove_rating_SMOTE_processed.keras'\n",
    "\n",
    "model.save(model_save_path)\n",
    "model.save(model_save_path_1)\n",
    "\n",
    "# plot training curve\n",
    "import matplotlib.pyplot as plt\n",
    "train_acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "\n",
    "plt.plot(train_acc, label = \"train_acc\")\n",
    "plt.plot(val_acc, label = \"val_acc\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# print classification report and confusion matrix\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "import numpy as np\n",
    "test_pred_class= y_pred.argmax(axis = 1)\n",
    "y_test_class = y_test\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test_class, test_pred_class))\n",
    "print(confusion_matrix(y_test_class, test_pred_class))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 'texts' contains the SMOTE-generated embeddings\n",
    "# 'X_embeddings' contains the original embeddings\n",
    "\n",
    "# Initialize the pre-trained sentence transformer model\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "\n",
    "\n",
    "# Extract the text data (preprocessed Text) and labels (ratings)\n",
    "X = df_2['text_processed'].tolist()\n",
    "y = df_2['rating'].tolist()\n",
    "\n",
    "# Encode the text data into sentence embeddings\n",
    "X_embeddings = model.encode(X)\n",
    "\n",
    "# Calculate cosine similarity between generated and original embeddings\n",
    "similarity_matrix = cosine_similarity(texts, X_embeddings)\n",
    "\n",
    "# For each generated embedding, find the index of the nearest neighbor\n",
    "nearest_neighbors = similarity_matrix.argmax(axis=1)\n",
    "\n",
    "# Retrieve the corresponding sentences from the original data\n",
    "generated_sentences = df_2['text_processed'].iloc[nearest_neighbors].tolist()\n",
    "\n",
    "# Print or inspect the generated sentences\n",
    "for sentence in generated_sentences:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
