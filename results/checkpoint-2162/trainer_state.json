{
  "best_metric": 0.6234967622571693,
  "best_model_checkpoint": "./results\\checkpoint-2162",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 2162,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.009250693802035153,
      "grad_norm": 1.6705536842346191,
      "learning_rate": 1.9938328707986433e-05,
      "loss": 1.6069,
      "step": 10
    },
    {
      "epoch": 0.018501387604070305,
      "grad_norm": 1.9342453479766846,
      "learning_rate": 1.9876657415972867e-05,
      "loss": 1.5653,
      "step": 20
    },
    {
      "epoch": 0.027752081406105456,
      "grad_norm": 2.567478656768799,
      "learning_rate": 1.98149861239593e-05,
      "loss": 1.5326,
      "step": 30
    },
    {
      "epoch": 0.03700277520814061,
      "grad_norm": 3.5929558277130127,
      "learning_rate": 1.9753314831945733e-05,
      "loss": 1.4502,
      "step": 40
    },
    {
      "epoch": 0.04625346901017576,
      "grad_norm": 3.8578920364379883,
      "learning_rate": 1.9691643539932164e-05,
      "loss": 1.3885,
      "step": 50
    },
    {
      "epoch": 0.05550416281221091,
      "grad_norm": 3.43536639213562,
      "learning_rate": 1.9629972247918595e-05,
      "loss": 1.3174,
      "step": 60
    },
    {
      "epoch": 0.06475485661424607,
      "grad_norm": 3.8894200325012207,
      "learning_rate": 1.956830095590503e-05,
      "loss": 1.2464,
      "step": 70
    },
    {
      "epoch": 0.07400555041628122,
      "grad_norm": 7.531097412109375,
      "learning_rate": 1.950662966389146e-05,
      "loss": 1.2351,
      "step": 80
    },
    {
      "epoch": 0.08325624421831637,
      "grad_norm": 6.534390449523926,
      "learning_rate": 1.9444958371877892e-05,
      "loss": 1.1807,
      "step": 90
    },
    {
      "epoch": 0.09250693802035152,
      "grad_norm": 4.135069370269775,
      "learning_rate": 1.9383287079864327e-05,
      "loss": 1.1263,
      "step": 100
    },
    {
      "epoch": 0.10175763182238667,
      "grad_norm": 4.378002643585205,
      "learning_rate": 1.9321615787850754e-05,
      "loss": 1.1815,
      "step": 110
    },
    {
      "epoch": 0.11100832562442182,
      "grad_norm": 6.793753623962402,
      "learning_rate": 1.925994449583719e-05,
      "loss": 1.087,
      "step": 120
    },
    {
      "epoch": 0.12025901942645699,
      "grad_norm": 9.974246978759766,
      "learning_rate": 1.919827320382362e-05,
      "loss": 1.0482,
      "step": 130
    },
    {
      "epoch": 0.12950971322849214,
      "grad_norm": 5.852987289428711,
      "learning_rate": 1.9136601911810055e-05,
      "loss": 1.0267,
      "step": 140
    },
    {
      "epoch": 0.13876040703052728,
      "grad_norm": 6.200011730194092,
      "learning_rate": 1.9074930619796486e-05,
      "loss": 1.0654,
      "step": 150
    },
    {
      "epoch": 0.14801110083256244,
      "grad_norm": 5.313354015350342,
      "learning_rate": 1.9013259327782917e-05,
      "loss": 0.9977,
      "step": 160
    },
    {
      "epoch": 0.1572617946345976,
      "grad_norm": 5.9940900802612305,
      "learning_rate": 1.895158803576935e-05,
      "loss": 1.082,
      "step": 170
    },
    {
      "epoch": 0.16651248843663274,
      "grad_norm": 9.85318660736084,
      "learning_rate": 1.8889916743755783e-05,
      "loss": 1.05,
      "step": 180
    },
    {
      "epoch": 0.1757631822386679,
      "grad_norm": 9.012782096862793,
      "learning_rate": 1.8828245451742214e-05,
      "loss": 1.0045,
      "step": 190
    },
    {
      "epoch": 0.18501387604070305,
      "grad_norm": 9.961283683776855,
      "learning_rate": 1.8766574159728648e-05,
      "loss": 1.1013,
      "step": 200
    },
    {
      "epoch": 0.1942645698427382,
      "grad_norm": 9.30042552947998,
      "learning_rate": 1.870490286771508e-05,
      "loss": 1.0146,
      "step": 210
    },
    {
      "epoch": 0.20351526364477335,
      "grad_norm": 7.797643184661865,
      "learning_rate": 1.8643231575701514e-05,
      "loss": 1.0199,
      "step": 220
    },
    {
      "epoch": 0.2127659574468085,
      "grad_norm": 7.553665637969971,
      "learning_rate": 1.8581560283687945e-05,
      "loss": 1.0963,
      "step": 230
    },
    {
      "epoch": 0.22201665124884365,
      "grad_norm": 9.525055885314941,
      "learning_rate": 1.8519888991674376e-05,
      "loss": 1.0081,
      "step": 240
    },
    {
      "epoch": 0.23126734505087881,
      "grad_norm": 8.441350936889648,
      "learning_rate": 1.845821769966081e-05,
      "loss": 1.0101,
      "step": 250
    },
    {
      "epoch": 0.24051803885291398,
      "grad_norm": 5.910520076751709,
      "learning_rate": 1.8396546407647242e-05,
      "loss": 1.0119,
      "step": 260
    },
    {
      "epoch": 0.24976873265494912,
      "grad_norm": 5.523601055145264,
      "learning_rate": 1.8334875115633673e-05,
      "loss": 1.0237,
      "step": 270
    },
    {
      "epoch": 0.2590194264569843,
      "grad_norm": 6.562713623046875,
      "learning_rate": 1.8273203823620108e-05,
      "loss": 0.9854,
      "step": 280
    },
    {
      "epoch": 0.2682701202590194,
      "grad_norm": 4.532113552093506,
      "learning_rate": 1.821153253160654e-05,
      "loss": 1.0899,
      "step": 290
    },
    {
      "epoch": 0.27752081406105455,
      "grad_norm": 8.08342170715332,
      "learning_rate": 1.8149861239592973e-05,
      "loss": 1.0362,
      "step": 300
    },
    {
      "epoch": 0.28677150786308975,
      "grad_norm": 5.078102111816406,
      "learning_rate": 1.8088189947579404e-05,
      "loss": 0.9739,
      "step": 310
    },
    {
      "epoch": 0.2960222016651249,
      "grad_norm": 4.894777297973633,
      "learning_rate": 1.8026518655565836e-05,
      "loss": 0.928,
      "step": 320
    },
    {
      "epoch": 0.30527289546716,
      "grad_norm": 7.318395614624023,
      "learning_rate": 1.7964847363552267e-05,
      "loss": 0.9429,
      "step": 330
    },
    {
      "epoch": 0.3145235892691952,
      "grad_norm": 9.170406341552734,
      "learning_rate": 1.7903176071538698e-05,
      "loss": 1.0737,
      "step": 340
    },
    {
      "epoch": 0.32377428307123035,
      "grad_norm": 5.219486713409424,
      "learning_rate": 1.7841504779525132e-05,
      "loss": 0.8507,
      "step": 350
    },
    {
      "epoch": 0.3330249768732655,
      "grad_norm": 5.886294364929199,
      "learning_rate": 1.7779833487511563e-05,
      "loss": 0.9905,
      "step": 360
    },
    {
      "epoch": 0.3422756706753006,
      "grad_norm": 5.010922431945801,
      "learning_rate": 1.7718162195497998e-05,
      "loss": 0.9172,
      "step": 370
    },
    {
      "epoch": 0.3515263644773358,
      "grad_norm": 4.117709159851074,
      "learning_rate": 1.765649090348443e-05,
      "loss": 0.9895,
      "step": 380
    },
    {
      "epoch": 0.36077705827937095,
      "grad_norm": 5.226416110992432,
      "learning_rate": 1.759481961147086e-05,
      "loss": 0.9252,
      "step": 390
    },
    {
      "epoch": 0.3700277520814061,
      "grad_norm": 7.320841312408447,
      "learning_rate": 1.7533148319457295e-05,
      "loss": 0.9276,
      "step": 400
    },
    {
      "epoch": 0.3792784458834413,
      "grad_norm": 13.586112976074219,
      "learning_rate": 1.7471477027443726e-05,
      "loss": 0.975,
      "step": 410
    },
    {
      "epoch": 0.3885291396854764,
      "grad_norm": 6.481622219085693,
      "learning_rate": 1.7409805735430157e-05,
      "loss": 1.017,
      "step": 420
    },
    {
      "epoch": 0.39777983348751156,
      "grad_norm": 7.7506022453308105,
      "learning_rate": 1.734813444341659e-05,
      "loss": 0.9801,
      "step": 430
    },
    {
      "epoch": 0.4070305272895467,
      "grad_norm": 6.34202241897583,
      "learning_rate": 1.7286463151403023e-05,
      "loss": 0.928,
      "step": 440
    },
    {
      "epoch": 0.4162812210915819,
      "grad_norm": 10.362092018127441,
      "learning_rate": 1.7224791859389457e-05,
      "loss": 0.9002,
      "step": 450
    },
    {
      "epoch": 0.425531914893617,
      "grad_norm": 4.842090606689453,
      "learning_rate": 1.716312056737589e-05,
      "loss": 1.0698,
      "step": 460
    },
    {
      "epoch": 0.43478260869565216,
      "grad_norm": 6.765182018280029,
      "learning_rate": 1.710144927536232e-05,
      "loss": 0.9485,
      "step": 470
    },
    {
      "epoch": 0.4440333024976873,
      "grad_norm": 8.182339668273926,
      "learning_rate": 1.7039777983348754e-05,
      "loss": 0.9734,
      "step": 480
    },
    {
      "epoch": 0.4532839962997225,
      "grad_norm": 7.449916362762451,
      "learning_rate": 1.6978106691335185e-05,
      "loss": 0.8629,
      "step": 490
    },
    {
      "epoch": 0.46253469010175763,
      "grad_norm": 8.823565483093262,
      "learning_rate": 1.6916435399321616e-05,
      "loss": 0.9475,
      "step": 500
    },
    {
      "epoch": 0.47178538390379277,
      "grad_norm": 5.6477484703063965,
      "learning_rate": 1.685476410730805e-05,
      "loss": 0.9995,
      "step": 510
    },
    {
      "epoch": 0.48103607770582796,
      "grad_norm": 11.911616325378418,
      "learning_rate": 1.6793092815294482e-05,
      "loss": 1.1141,
      "step": 520
    },
    {
      "epoch": 0.4902867715078631,
      "grad_norm": 6.71022367477417,
      "learning_rate": 1.6731421523280917e-05,
      "loss": 0.9581,
      "step": 530
    },
    {
      "epoch": 0.49953746530989823,
      "grad_norm": 7.09035062789917,
      "learning_rate": 1.6669750231267344e-05,
      "loss": 0.9891,
      "step": 540
    },
    {
      "epoch": 0.5087881591119334,
      "grad_norm": 8.048428535461426,
      "learning_rate": 1.660807893925378e-05,
      "loss": 1.0759,
      "step": 550
    },
    {
      "epoch": 0.5180388529139686,
      "grad_norm": 9.248451232910156,
      "learning_rate": 1.654640764724021e-05,
      "loss": 1.0644,
      "step": 560
    },
    {
      "epoch": 0.5272895467160037,
      "grad_norm": 5.448602676391602,
      "learning_rate": 1.648473635522664e-05,
      "loss": 0.8829,
      "step": 570
    },
    {
      "epoch": 0.5365402405180388,
      "grad_norm": 6.147519588470459,
      "learning_rate": 1.6423065063213076e-05,
      "loss": 0.9111,
      "step": 580
    },
    {
      "epoch": 0.545790934320074,
      "grad_norm": 6.7476420402526855,
      "learning_rate": 1.6361393771199507e-05,
      "loss": 0.9738,
      "step": 590
    },
    {
      "epoch": 0.5550416281221091,
      "grad_norm": 4.150820255279541,
      "learning_rate": 1.629972247918594e-05,
      "loss": 0.9181,
      "step": 600
    },
    {
      "epoch": 0.5642923219241444,
      "grad_norm": 7.627056121826172,
      "learning_rate": 1.6238051187172373e-05,
      "loss": 1.0008,
      "step": 610
    },
    {
      "epoch": 0.5735430157261795,
      "grad_norm": 8.841943740844727,
      "learning_rate": 1.6176379895158804e-05,
      "loss": 0.9019,
      "step": 620
    },
    {
      "epoch": 0.5827937095282146,
      "grad_norm": 9.067590713500977,
      "learning_rate": 1.6114708603145238e-05,
      "loss": 1.0138,
      "step": 630
    },
    {
      "epoch": 0.5920444033302498,
      "grad_norm": 7.820014953613281,
      "learning_rate": 1.605303731113167e-05,
      "loss": 0.9306,
      "step": 640
    },
    {
      "epoch": 0.6012950971322849,
      "grad_norm": 6.033749580383301,
      "learning_rate": 1.59913660191181e-05,
      "loss": 0.7846,
      "step": 650
    },
    {
      "epoch": 0.61054579093432,
      "grad_norm": 10.941455841064453,
      "learning_rate": 1.5929694727104535e-05,
      "loss": 1.065,
      "step": 660
    },
    {
      "epoch": 0.6197964847363552,
      "grad_norm": 5.660898685455322,
      "learning_rate": 1.5868023435090966e-05,
      "loss": 0.889,
      "step": 670
    },
    {
      "epoch": 0.6290471785383904,
      "grad_norm": 9.862699508666992,
      "learning_rate": 1.58063521430774e-05,
      "loss": 0.8274,
      "step": 680
    },
    {
      "epoch": 0.6382978723404256,
      "grad_norm": 7.462131023406982,
      "learning_rate": 1.5744680851063832e-05,
      "loss": 0.9906,
      "step": 690
    },
    {
      "epoch": 0.6475485661424607,
      "grad_norm": 4.875250816345215,
      "learning_rate": 1.5683009559050263e-05,
      "loss": 0.8212,
      "step": 700
    },
    {
      "epoch": 0.6567992599444958,
      "grad_norm": 7.873554706573486,
      "learning_rate": 1.5621338267036698e-05,
      "loss": 0.95,
      "step": 710
    },
    {
      "epoch": 0.666049953746531,
      "grad_norm": 5.833280563354492,
      "learning_rate": 1.555966697502313e-05,
      "loss": 0.9477,
      "step": 720
    },
    {
      "epoch": 0.6753006475485661,
      "grad_norm": 8.109892845153809,
      "learning_rate": 1.549799568300956e-05,
      "loss": 0.8461,
      "step": 730
    },
    {
      "epoch": 0.6845513413506013,
      "grad_norm": 7.213862419128418,
      "learning_rate": 1.5436324390995994e-05,
      "loss": 0.9765,
      "step": 740
    },
    {
      "epoch": 0.6938020351526365,
      "grad_norm": 6.489466190338135,
      "learning_rate": 1.5374653098982426e-05,
      "loss": 1.0066,
      "step": 750
    },
    {
      "epoch": 0.7030527289546716,
      "grad_norm": 7.322519302368164,
      "learning_rate": 1.5312981806968857e-05,
      "loss": 0.8897,
      "step": 760
    },
    {
      "epoch": 0.7123034227567068,
      "grad_norm": 7.157871246337891,
      "learning_rate": 1.525131051495529e-05,
      "loss": 1.0062,
      "step": 770
    },
    {
      "epoch": 0.7215541165587419,
      "grad_norm": 5.193210601806641,
      "learning_rate": 1.518963922294172e-05,
      "loss": 0.8818,
      "step": 780
    },
    {
      "epoch": 0.730804810360777,
      "grad_norm": 8.206948280334473,
      "learning_rate": 1.5127967930928153e-05,
      "loss": 0.9597,
      "step": 790
    },
    {
      "epoch": 0.7400555041628122,
      "grad_norm": 5.838427543640137,
      "learning_rate": 1.5066296638914586e-05,
      "loss": 0.8272,
      "step": 800
    },
    {
      "epoch": 0.7493061979648473,
      "grad_norm": 10.746663093566895,
      "learning_rate": 1.5004625346901019e-05,
      "loss": 1.0236,
      "step": 810
    },
    {
      "epoch": 0.7585568917668826,
      "grad_norm": 4.757213592529297,
      "learning_rate": 1.494295405488745e-05,
      "loss": 0.9746,
      "step": 820
    },
    {
      "epoch": 0.7678075855689177,
      "grad_norm": 5.623650550842285,
      "learning_rate": 1.4881282762873883e-05,
      "loss": 1.0109,
      "step": 830
    },
    {
      "epoch": 0.7770582793709528,
      "grad_norm": 7.005825042724609,
      "learning_rate": 1.4819611470860316e-05,
      "loss": 0.8814,
      "step": 840
    },
    {
      "epoch": 0.786308973172988,
      "grad_norm": 5.3169846534729,
      "learning_rate": 1.4757940178846749e-05,
      "loss": 0.9242,
      "step": 850
    },
    {
      "epoch": 0.7955596669750231,
      "grad_norm": 5.931900978088379,
      "learning_rate": 1.469626888683318e-05,
      "loss": 0.886,
      "step": 860
    },
    {
      "epoch": 0.8048103607770583,
      "grad_norm": 5.171823501586914,
      "learning_rate": 1.4634597594819613e-05,
      "loss": 0.9251,
      "step": 870
    },
    {
      "epoch": 0.8140610545790934,
      "grad_norm": 8.6849365234375,
      "learning_rate": 1.4572926302806046e-05,
      "loss": 0.8948,
      "step": 880
    },
    {
      "epoch": 0.8233117483811286,
      "grad_norm": 6.407121181488037,
      "learning_rate": 1.4511255010792478e-05,
      "loss": 0.9476,
      "step": 890
    },
    {
      "epoch": 0.8325624421831638,
      "grad_norm": 8.98587703704834,
      "learning_rate": 1.444958371877891e-05,
      "loss": 0.8689,
      "step": 900
    },
    {
      "epoch": 0.8418131359851989,
      "grad_norm": 8.256662368774414,
      "learning_rate": 1.4387912426765342e-05,
      "loss": 0.8985,
      "step": 910
    },
    {
      "epoch": 0.851063829787234,
      "grad_norm": 5.9156928062438965,
      "learning_rate": 1.4326241134751775e-05,
      "loss": 0.9544,
      "step": 920
    },
    {
      "epoch": 0.8603145235892692,
      "grad_norm": 8.78812313079834,
      "learning_rate": 1.4264569842738208e-05,
      "loss": 0.9036,
      "step": 930
    },
    {
      "epoch": 0.8695652173913043,
      "grad_norm": 7.050388336181641,
      "learning_rate": 1.420289855072464e-05,
      "loss": 0.8694,
      "step": 940
    },
    {
      "epoch": 0.8788159111933395,
      "grad_norm": 7.650028228759766,
      "learning_rate": 1.4141227258711072e-05,
      "loss": 0.9297,
      "step": 950
    },
    {
      "epoch": 0.8880666049953746,
      "grad_norm": 5.928703784942627,
      "learning_rate": 1.4079555966697505e-05,
      "loss": 0.9156,
      "step": 960
    },
    {
      "epoch": 0.8973172987974098,
      "grad_norm": 5.684692859649658,
      "learning_rate": 1.4017884674683934e-05,
      "loss": 0.8834,
      "step": 970
    },
    {
      "epoch": 0.906567992599445,
      "grad_norm": 4.7376580238342285,
      "learning_rate": 1.3956213382670367e-05,
      "loss": 0.9165,
      "step": 980
    },
    {
      "epoch": 0.9158186864014801,
      "grad_norm": 5.589293003082275,
      "learning_rate": 1.38945420906568e-05,
      "loss": 0.9516,
      "step": 990
    },
    {
      "epoch": 0.9250693802035153,
      "grad_norm": 5.696262359619141,
      "learning_rate": 1.3832870798643231e-05,
      "loss": 0.8753,
      "step": 1000
    },
    {
      "epoch": 0.9343200740055504,
      "grad_norm": 8.733871459960938,
      "learning_rate": 1.3771199506629664e-05,
      "loss": 1.017,
      "step": 1010
    },
    {
      "epoch": 0.9435707678075855,
      "grad_norm": 8.494422912597656,
      "learning_rate": 1.3709528214616097e-05,
      "loss": 0.9815,
      "step": 1020
    },
    {
      "epoch": 0.9528214616096207,
      "grad_norm": 7.432690620422363,
      "learning_rate": 1.364785692260253e-05,
      "loss": 0.905,
      "step": 1030
    },
    {
      "epoch": 0.9620721554116559,
      "grad_norm": 4.226840972900391,
      "learning_rate": 1.3586185630588961e-05,
      "loss": 0.9359,
      "step": 1040
    },
    {
      "epoch": 0.971322849213691,
      "grad_norm": 6.8733954429626465,
      "learning_rate": 1.3524514338575394e-05,
      "loss": 0.8648,
      "step": 1050
    },
    {
      "epoch": 0.9805735430157262,
      "grad_norm": 5.472556114196777,
      "learning_rate": 1.3462843046561827e-05,
      "loss": 0.9136,
      "step": 1060
    },
    {
      "epoch": 0.9898242368177613,
      "grad_norm": 3.7882347106933594,
      "learning_rate": 1.340117175454826e-05,
      "loss": 0.9345,
      "step": 1070
    },
    {
      "epoch": 0.9990749306197965,
      "grad_norm": 5.757761001586914,
      "learning_rate": 1.333950046253469e-05,
      "loss": 0.8401,
      "step": 1080
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.6112395929694727,
      "eval_loss": 0.9159550070762634,
      "eval_runtime": 743.81,
      "eval_samples_per_second": 5.813,
      "eval_steps_per_second": 0.364,
      "step": 1081
    },
    {
      "epoch": 1.0083256244218317,
      "grad_norm": 6.839674949645996,
      "learning_rate": 1.3277829170521123e-05,
      "loss": 0.7582,
      "step": 1090
    },
    {
      "epoch": 1.0175763182238668,
      "grad_norm": 6.376169204711914,
      "learning_rate": 1.3216157878507556e-05,
      "loss": 0.8302,
      "step": 1100
    },
    {
      "epoch": 1.026827012025902,
      "grad_norm": 7.406371593475342,
      "learning_rate": 1.3154486586493989e-05,
      "loss": 0.7179,
      "step": 1110
    },
    {
      "epoch": 1.0360777058279371,
      "grad_norm": 6.388751983642578,
      "learning_rate": 1.309281529448042e-05,
      "loss": 0.8466,
      "step": 1120
    },
    {
      "epoch": 1.0453283996299723,
      "grad_norm": 4.311958312988281,
      "learning_rate": 1.3031144002466853e-05,
      "loss": 0.8316,
      "step": 1130
    },
    {
      "epoch": 1.0545790934320074,
      "grad_norm": 7.283366680145264,
      "learning_rate": 1.2969472710453286e-05,
      "loss": 0.8909,
      "step": 1140
    },
    {
      "epoch": 1.0638297872340425,
      "grad_norm": 5.722158908843994,
      "learning_rate": 1.2907801418439719e-05,
      "loss": 0.919,
      "step": 1150
    },
    {
      "epoch": 1.0730804810360777,
      "grad_norm": 5.251672744750977,
      "learning_rate": 1.284613012642615e-05,
      "loss": 0.8386,
      "step": 1160
    },
    {
      "epoch": 1.0823311748381128,
      "grad_norm": 8.446351051330566,
      "learning_rate": 1.2784458834412583e-05,
      "loss": 0.8568,
      "step": 1170
    },
    {
      "epoch": 1.091581868640148,
      "grad_norm": 6.056087017059326,
      "learning_rate": 1.2722787542399014e-05,
      "loss": 0.7388,
      "step": 1180
    },
    {
      "epoch": 1.100832562442183,
      "grad_norm": 6.042623996734619,
      "learning_rate": 1.2661116250385445e-05,
      "loss": 0.7303,
      "step": 1190
    },
    {
      "epoch": 1.1100832562442182,
      "grad_norm": 11.958532333374023,
      "learning_rate": 1.2599444958371878e-05,
      "loss": 0.8368,
      "step": 1200
    },
    {
      "epoch": 1.1193339500462534,
      "grad_norm": 9.707469940185547,
      "learning_rate": 1.253777366635831e-05,
      "loss": 0.7533,
      "step": 1210
    },
    {
      "epoch": 1.1285846438482887,
      "grad_norm": 9.419227600097656,
      "learning_rate": 1.2476102374344743e-05,
      "loss": 0.7093,
      "step": 1220
    },
    {
      "epoch": 1.1378353376503239,
      "grad_norm": 4.8534159660339355,
      "learning_rate": 1.2414431082331175e-05,
      "loss": 0.6418,
      "step": 1230
    },
    {
      "epoch": 1.147086031452359,
      "grad_norm": 6.692648410797119,
      "learning_rate": 1.2352759790317607e-05,
      "loss": 0.9143,
      "step": 1240
    },
    {
      "epoch": 1.1563367252543941,
      "grad_norm": 5.084749698638916,
      "learning_rate": 1.229108849830404e-05,
      "loss": 0.8922,
      "step": 1250
    },
    {
      "epoch": 1.1655874190564293,
      "grad_norm": 7.135986804962158,
      "learning_rate": 1.2229417206290473e-05,
      "loss": 0.7942,
      "step": 1260
    },
    {
      "epoch": 1.1748381128584644,
      "grad_norm": 6.125086784362793,
      "learning_rate": 1.2167745914276904e-05,
      "loss": 0.8479,
      "step": 1270
    },
    {
      "epoch": 1.1840888066604995,
      "grad_norm": 8.850558280944824,
      "learning_rate": 1.2106074622263337e-05,
      "loss": 0.8358,
      "step": 1280
    },
    {
      "epoch": 1.1933395004625347,
      "grad_norm": 8.333456039428711,
      "learning_rate": 1.204440333024977e-05,
      "loss": 0.7726,
      "step": 1290
    },
    {
      "epoch": 1.2025901942645698,
      "grad_norm": 7.109560966491699,
      "learning_rate": 1.1982732038236203e-05,
      "loss": 0.8392,
      "step": 1300
    },
    {
      "epoch": 1.211840888066605,
      "grad_norm": 6.161740303039551,
      "learning_rate": 1.1921060746222634e-05,
      "loss": 0.6985,
      "step": 1310
    },
    {
      "epoch": 1.22109158186864,
      "grad_norm": 4.998850345611572,
      "learning_rate": 1.1859389454209067e-05,
      "loss": 0.7098,
      "step": 1320
    },
    {
      "epoch": 1.2303422756706752,
      "grad_norm": 7.434821128845215,
      "learning_rate": 1.17977181621955e-05,
      "loss": 0.7347,
      "step": 1330
    },
    {
      "epoch": 1.2395929694727104,
      "grad_norm": 8.756418228149414,
      "learning_rate": 1.1736046870181932e-05,
      "loss": 0.8122,
      "step": 1340
    },
    {
      "epoch": 1.2488436632747457,
      "grad_norm": 8.757704734802246,
      "learning_rate": 1.1674375578168364e-05,
      "loss": 0.8152,
      "step": 1350
    },
    {
      "epoch": 1.2580943570767809,
      "grad_norm": 9.475339889526367,
      "learning_rate": 1.1612704286154796e-05,
      "loss": 0.7868,
      "step": 1360
    },
    {
      "epoch": 1.267345050878816,
      "grad_norm": 4.365220546722412,
      "learning_rate": 1.155103299414123e-05,
      "loss": 0.7926,
      "step": 1370
    },
    {
      "epoch": 1.2765957446808511,
      "grad_norm": 6.094344139099121,
      "learning_rate": 1.1489361702127662e-05,
      "loss": 0.7775,
      "step": 1380
    },
    {
      "epoch": 1.2858464384828863,
      "grad_norm": 9.234994888305664,
      "learning_rate": 1.1427690410114092e-05,
      "loss": 0.7132,
      "step": 1390
    },
    {
      "epoch": 1.2950971322849214,
      "grad_norm": 9.154414176940918,
      "learning_rate": 1.1366019118100524e-05,
      "loss": 0.8231,
      "step": 1400
    },
    {
      "epoch": 1.3043478260869565,
      "grad_norm": 8.430617332458496,
      "learning_rate": 1.1304347826086957e-05,
      "loss": 0.8506,
      "step": 1410
    },
    {
      "epoch": 1.3135985198889917,
      "grad_norm": 9.893769264221191,
      "learning_rate": 1.1242676534073388e-05,
      "loss": 0.8476,
      "step": 1420
    },
    {
      "epoch": 1.3228492136910268,
      "grad_norm": 6.417590141296387,
      "learning_rate": 1.1181005242059821e-05,
      "loss": 0.7727,
      "step": 1430
    },
    {
      "epoch": 1.332099907493062,
      "grad_norm": 7.575127601623535,
      "learning_rate": 1.1119333950046254e-05,
      "loss": 0.7809,
      "step": 1440
    },
    {
      "epoch": 1.341350601295097,
      "grad_norm": 8.478399276733398,
      "learning_rate": 1.1057662658032687e-05,
      "loss": 0.7821,
      "step": 1450
    },
    {
      "epoch": 1.3506012950971322,
      "grad_norm": 8.987105369567871,
      "learning_rate": 1.0995991366019118e-05,
      "loss": 0.7774,
      "step": 1460
    },
    {
      "epoch": 1.3598519888991674,
      "grad_norm": 10.435037612915039,
      "learning_rate": 1.0934320074005551e-05,
      "loss": 0.7655,
      "step": 1470
    },
    {
      "epoch": 1.3691026827012025,
      "grad_norm": 5.454435348510742,
      "learning_rate": 1.0872648781991984e-05,
      "loss": 0.7696,
      "step": 1480
    },
    {
      "epoch": 1.3783533765032376,
      "grad_norm": 8.643383026123047,
      "learning_rate": 1.0810977489978417e-05,
      "loss": 0.8829,
      "step": 1490
    },
    {
      "epoch": 1.3876040703052728,
      "grad_norm": 5.579920768737793,
      "learning_rate": 1.0749306197964848e-05,
      "loss": 0.7256,
      "step": 1500
    },
    {
      "epoch": 1.396854764107308,
      "grad_norm": 10.530611991882324,
      "learning_rate": 1.068763490595128e-05,
      "loss": 0.8755,
      "step": 1510
    },
    {
      "epoch": 1.4061054579093433,
      "grad_norm": 10.463446617126465,
      "learning_rate": 1.0625963613937713e-05,
      "loss": 0.9061,
      "step": 1520
    },
    {
      "epoch": 1.4153561517113784,
      "grad_norm": 8.074493408203125,
      "learning_rate": 1.0564292321924146e-05,
      "loss": 0.7333,
      "step": 1530
    },
    {
      "epoch": 1.4246068455134135,
      "grad_norm": 12.144375801086426,
      "learning_rate": 1.0502621029910577e-05,
      "loss": 0.7857,
      "step": 1540
    },
    {
      "epoch": 1.4338575393154487,
      "grad_norm": 8.154037475585938,
      "learning_rate": 1.044094973789701e-05,
      "loss": 0.8999,
      "step": 1550
    },
    {
      "epoch": 1.4431082331174838,
      "grad_norm": 7.2084479331970215,
      "learning_rate": 1.0379278445883443e-05,
      "loss": 0.6493,
      "step": 1560
    },
    {
      "epoch": 1.452358926919519,
      "grad_norm": 6.699007511138916,
      "learning_rate": 1.0317607153869876e-05,
      "loss": 0.7251,
      "step": 1570
    },
    {
      "epoch": 1.461609620721554,
      "grad_norm": 8.68899917602539,
      "learning_rate": 1.0255935861856307e-05,
      "loss": 0.6853,
      "step": 1580
    },
    {
      "epoch": 1.4708603145235892,
      "grad_norm": 6.988973617553711,
      "learning_rate": 1.019426456984274e-05,
      "loss": 0.7621,
      "step": 1590
    },
    {
      "epoch": 1.4801110083256244,
      "grad_norm": 5.842775821685791,
      "learning_rate": 1.0132593277829173e-05,
      "loss": 0.9465,
      "step": 1600
    },
    {
      "epoch": 1.4893617021276595,
      "grad_norm": 8.702909469604492,
      "learning_rate": 1.0070921985815602e-05,
      "loss": 0.7618,
      "step": 1610
    },
    {
      "epoch": 1.4986123959296949,
      "grad_norm": 7.18402099609375,
      "learning_rate": 1.0009250693802035e-05,
      "loss": 0.762,
      "step": 1620
    },
    {
      "epoch": 1.50786308973173,
      "grad_norm": 7.280163764953613,
      "learning_rate": 9.947579401788468e-06,
      "loss": 0.7815,
      "step": 1630
    },
    {
      "epoch": 1.5171137835337651,
      "grad_norm": 5.9464335441589355,
      "learning_rate": 9.8859081097749e-06,
      "loss": 0.7618,
      "step": 1640
    },
    {
      "epoch": 1.5263644773358003,
      "grad_norm": 4.748244285583496,
      "learning_rate": 9.824236817761333e-06,
      "loss": 0.8449,
      "step": 1650
    },
    {
      "epoch": 1.5356151711378354,
      "grad_norm": 7.000027179718018,
      "learning_rate": 9.762565525747765e-06,
      "loss": 0.9045,
      "step": 1660
    },
    {
      "epoch": 1.5448658649398705,
      "grad_norm": 9.076605796813965,
      "learning_rate": 9.700894233734197e-06,
      "loss": 0.8889,
      "step": 1670
    },
    {
      "epoch": 1.5541165587419057,
      "grad_norm": 7.557755470275879,
      "learning_rate": 9.63922294172063e-06,
      "loss": 0.8152,
      "step": 1680
    },
    {
      "epoch": 1.5633672525439408,
      "grad_norm": 6.081655025482178,
      "learning_rate": 9.577551649707061e-06,
      "loss": 0.7886,
      "step": 1690
    },
    {
      "epoch": 1.572617946345976,
      "grad_norm": 6.820692539215088,
      "learning_rate": 9.515880357693494e-06,
      "loss": 0.6947,
      "step": 1700
    },
    {
      "epoch": 1.581868640148011,
      "grad_norm": 11.29126262664795,
      "learning_rate": 9.454209065679927e-06,
      "loss": 0.8072,
      "step": 1710
    },
    {
      "epoch": 1.5911193339500462,
      "grad_norm": 7.55873441696167,
      "learning_rate": 9.39253777366636e-06,
      "loss": 0.7749,
      "step": 1720
    },
    {
      "epoch": 1.6003700277520814,
      "grad_norm": 9.603344917297363,
      "learning_rate": 9.330866481652791e-06,
      "loss": 0.8745,
      "step": 1730
    },
    {
      "epoch": 1.6096207215541165,
      "grad_norm": 8.831040382385254,
      "learning_rate": 9.269195189639224e-06,
      "loss": 0.8608,
      "step": 1740
    },
    {
      "epoch": 1.6188714153561516,
      "grad_norm": 10.730672836303711,
      "learning_rate": 9.207523897625657e-06,
      "loss": 0.8845,
      "step": 1750
    },
    {
      "epoch": 1.6281221091581868,
      "grad_norm": 6.981279373168945,
      "learning_rate": 9.14585260561209e-06,
      "loss": 0.8511,
      "step": 1760
    },
    {
      "epoch": 1.637372802960222,
      "grad_norm": 7.685239315032959,
      "learning_rate": 9.08418131359852e-06,
      "loss": 0.7243,
      "step": 1770
    },
    {
      "epoch": 1.646623496762257,
      "grad_norm": 6.274316310882568,
      "learning_rate": 9.022510021584952e-06,
      "loss": 0.6767,
      "step": 1780
    },
    {
      "epoch": 1.6558741905642922,
      "grad_norm": 6.57741117477417,
      "learning_rate": 8.960838729571385e-06,
      "loss": 0.8359,
      "step": 1790
    },
    {
      "epoch": 1.6651248843663273,
      "grad_norm": 5.649810791015625,
      "learning_rate": 8.899167437557818e-06,
      "loss": 0.6755,
      "step": 1800
    },
    {
      "epoch": 1.6743755781683625,
      "grad_norm": 15.030658721923828,
      "learning_rate": 8.83749614554425e-06,
      "loss": 0.7916,
      "step": 1810
    },
    {
      "epoch": 1.6836262719703978,
      "grad_norm": 15.125415802001953,
      "learning_rate": 8.775824853530682e-06,
      "loss": 0.8631,
      "step": 1820
    },
    {
      "epoch": 1.692876965772433,
      "grad_norm": 9.262378692626953,
      "learning_rate": 8.714153561517114e-06,
      "loss": 0.7959,
      "step": 1830
    },
    {
      "epoch": 1.702127659574468,
      "grad_norm": 8.550655364990234,
      "learning_rate": 8.652482269503547e-06,
      "loss": 0.7283,
      "step": 1840
    },
    {
      "epoch": 1.7113783533765032,
      "grad_norm": 7.848727703094482,
      "learning_rate": 8.59081097748998e-06,
      "loss": 0.9613,
      "step": 1850
    },
    {
      "epoch": 1.7206290471785384,
      "grad_norm": 6.502878665924072,
      "learning_rate": 8.529139685476411e-06,
      "loss": 0.8404,
      "step": 1860
    },
    {
      "epoch": 1.7298797409805735,
      "grad_norm": 7.009759426116943,
      "learning_rate": 8.467468393462844e-06,
      "loss": 0.8147,
      "step": 1870
    },
    {
      "epoch": 1.7391304347826086,
      "grad_norm": 8.795655250549316,
      "learning_rate": 8.405797101449275e-06,
      "loss": 0.8618,
      "step": 1880
    },
    {
      "epoch": 1.748381128584644,
      "grad_norm": 5.943448066711426,
      "learning_rate": 8.344125809435708e-06,
      "loss": 0.7424,
      "step": 1890
    },
    {
      "epoch": 1.7576318223866791,
      "grad_norm": 6.011892318725586,
      "learning_rate": 8.282454517422141e-06,
      "loss": 0.7141,
      "step": 1900
    },
    {
      "epoch": 1.7668825161887143,
      "grad_norm": 6.517746925354004,
      "learning_rate": 8.220783225408574e-06,
      "loss": 0.8412,
      "step": 1910
    },
    {
      "epoch": 1.7761332099907494,
      "grad_norm": 8.811437606811523,
      "learning_rate": 8.159111933395005e-06,
      "loss": 0.8954,
      "step": 1920
    },
    {
      "epoch": 1.7853839037927846,
      "grad_norm": 10.548059463500977,
      "learning_rate": 8.097440641381438e-06,
      "loss": 0.7855,
      "step": 1930
    },
    {
      "epoch": 1.7946345975948197,
      "grad_norm": 8.170812606811523,
      "learning_rate": 8.03576934936787e-06,
      "loss": 0.7222,
      "step": 1940
    },
    {
      "epoch": 1.8038852913968548,
      "grad_norm": 9.255223274230957,
      "learning_rate": 7.974098057354303e-06,
      "loss": 0.826,
      "step": 1950
    },
    {
      "epoch": 1.81313598519889,
      "grad_norm": 8.164468765258789,
      "learning_rate": 7.912426765340735e-06,
      "loss": 0.7466,
      "step": 1960
    },
    {
      "epoch": 1.822386679000925,
      "grad_norm": 12.00171184539795,
      "learning_rate": 7.850755473327167e-06,
      "loss": 0.8785,
      "step": 1970
    },
    {
      "epoch": 1.8316373728029602,
      "grad_norm": 5.520756244659424,
      "learning_rate": 7.789084181313598e-06,
      "loss": 0.9445,
      "step": 1980
    },
    {
      "epoch": 1.8408880666049954,
      "grad_norm": 5.836824417114258,
      "learning_rate": 7.727412889300031e-06,
      "loss": 0.7294,
      "step": 1990
    },
    {
      "epoch": 1.8501387604070305,
      "grad_norm": 8.362072944641113,
      "learning_rate": 7.665741597286464e-06,
      "loss": 0.7983,
      "step": 2000
    },
    {
      "epoch": 1.8593894542090657,
      "grad_norm": 10.936583518981934,
      "learning_rate": 7.604070305272896e-06,
      "loss": 0.7706,
      "step": 2010
    },
    {
      "epoch": 1.8686401480111008,
      "grad_norm": 6.204894065856934,
      "learning_rate": 7.542399013259328e-06,
      "loss": 0.8286,
      "step": 2020
    },
    {
      "epoch": 1.877890841813136,
      "grad_norm": 9.272188186645508,
      "learning_rate": 7.480727721245761e-06,
      "loss": 0.7346,
      "step": 2030
    },
    {
      "epoch": 1.887141535615171,
      "grad_norm": 11.038840293884277,
      "learning_rate": 7.419056429232193e-06,
      "loss": 0.7969,
      "step": 2040
    },
    {
      "epoch": 1.8963922294172062,
      "grad_norm": 14.038978576660156,
      "learning_rate": 7.357385137218626e-06,
      "loss": 0.7398,
      "step": 2050
    },
    {
      "epoch": 1.9056429232192413,
      "grad_norm": 6.802778720855713,
      "learning_rate": 7.295713845205058e-06,
      "loss": 0.6892,
      "step": 2060
    },
    {
      "epoch": 1.9148936170212765,
      "grad_norm": 5.731940269470215,
      "learning_rate": 7.234042553191491e-06,
      "loss": 0.7727,
      "step": 2070
    },
    {
      "epoch": 1.9241443108233116,
      "grad_norm": 9.25487995147705,
      "learning_rate": 7.172371261177923e-06,
      "loss": 0.8305,
      "step": 2080
    },
    {
      "epoch": 1.9333950046253467,
      "grad_norm": 10.062543869018555,
      "learning_rate": 7.110699969164354e-06,
      "loss": 0.8409,
      "step": 2090
    },
    {
      "epoch": 1.942645698427382,
      "grad_norm": 7.362149715423584,
      "learning_rate": 7.049028677150787e-06,
      "loss": 0.753,
      "step": 2100
    },
    {
      "epoch": 1.9518963922294172,
      "grad_norm": 5.973982810974121,
      "learning_rate": 6.987357385137219e-06,
      "loss": 0.7423,
      "step": 2110
    },
    {
      "epoch": 1.9611470860314524,
      "grad_norm": 7.985396385192871,
      "learning_rate": 6.9256860931236514e-06,
      "loss": 0.8206,
      "step": 2120
    },
    {
      "epoch": 1.9703977798334875,
      "grad_norm": 8.875699996948242,
      "learning_rate": 6.8640148011100834e-06,
      "loss": 0.8563,
      "step": 2130
    },
    {
      "epoch": 1.9796484736355227,
      "grad_norm": 5.974417686462402,
      "learning_rate": 6.802343509096516e-06,
      "loss": 0.7679,
      "step": 2140
    },
    {
      "epoch": 1.9888991674375578,
      "grad_norm": 6.135342121124268,
      "learning_rate": 6.740672217082948e-06,
      "loss": 0.7113,
      "step": 2150
    },
    {
      "epoch": 1.998149861239593,
      "grad_norm": 7.420351028442383,
      "learning_rate": 6.679000925069381e-06,
      "loss": 0.7445,
      "step": 2160
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.6234967622571693,
      "eval_loss": 0.9004201292991638,
      "eval_runtime": 760.1492,
      "eval_samples_per_second": 5.688,
      "eval_steps_per_second": 0.357,
      "step": 2162
    }
  ],
  "logging_steps": 10,
  "max_steps": 3243,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1145639352852480.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
